import streamlit as st
import groq
import os
from typing import List, Dict, Optional, Union
from dotenv import load_dotenv
import time
import base64
import importlib.util
from datetime import datetime

# Verificar si pydantic_ai est√° instalado
pydantic_available = importlib.util.find_spec("pydantic_ai") is not None

# Verificar si tavily-python est√° instalado
tavily_available = importlib.util.find_spec("tavily") is not None

# Verificar si duckduckgo-search est√° instalado
duckduckgo_available = importlib.util.find_spec("duckduckgo_search") is not None

# Verificar si nest_asyncio est√° instalado (para entornos Jupyter)
nest_asyncio_available = importlib.util.find_spec("nest_asyncio") is not None

# Importar pydantic_ai y nest_asyncio si est√°n disponibles
if pydantic_available:
    try:
        from pydantic_ai import Agent, RunContext
        from pydantic import BaseModel
        # No importamos directamente duckduckgo_search_tool debido a incompatibilidad con Python 3.9
        # Usaremos nuestra propia implementaci√≥n para b√∫squeda DuckDuckGo
        # Aplicar nest_asyncio si est√° disponible (para compatibilidad con Jupyter)
        if nest_asyncio_available:
            import nest_asyncio
            nest_asyncio.apply()
    except ImportError:
        pydantic_available = False

# Importar tavily-python si est√° disponible
if tavily_available:
    try:
        from tavily import TavilyClient
    except ImportError:
        tavily_available = False

# Cargar variables de entorno desde .env
load_dotenv()

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Asistente IA con Groq",
    page_icon="ü§ñ",
    layout="centered"
)

# Estilo CSS personalizado
st.markdown("""
<style>
/* Estilos generales para los contenedores de chat */
.chat-container {
    border-radius: 12px;
    margin-bottom: 15px;
    padding: 15px;
    box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    border: 1px solid rgba(0,0,0,0.05);
}

/* Estilos para mensajes del usuario */
.user-message {
    background-color: var(--primary-color-light);
    text-align: right;
    color: var(--text-color);
    font-weight: 500;
    margin-left: 20%;
}

/* Estilos para mensajes del asistente */
.assistant-message {
    background-color: var(--secondary-background-color);
    color: var(--text-color);
    font-weight: 500;
    margin-right: 20%;
}

/* Estilos para el texto del chat */
.chat-text {
    font-size: 16px;
    line-height: 1.5;
    color: var(--text-color) !important;
}

/* Estilos para encabezados de los mensajes */
.user-header, .assistant-header {
    font-weight: 700;
    margin-bottom: 5px;
}

.user-header {
    color: var(--primary-color);
}

.assistant-header {
    color: var(--text-color);
}

/* Ajustes para la zona de entrada de texto */
.stTextInput input {
    border-radius: 20px;
    padding: 10px 15px;
}

/* Estilos para la secci√≥n de razonamiento */
.reasoning-section {
    background-color: rgba(var(--primary-color-rgb, 75, 75, 255), 0.05);
    border-radius: 8px;
    padding: 15px;
    margin: 10px 0 15px 0;
    border-left: 4px solid var(--primary-color);
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.thinking-title {
    font-weight: 700;
    color: var(--primary-color);
    margin-bottom: 8px;
    font-size: 14px;
    text-transform: uppercase;
    letter-spacing: 0.5px;
    display: flex;
    align-items: center;
}

.thinking-title:before {
    content: "üí≠";
    margin-right: 6px;
}

/* Contenido del razonamiento con estilo diferenciado */
.reasoning-content {
    font-family: monospace;
    font-size: 14px;
    line-height: 1.6;
    color: var(--text-color);
    opacity: 0.85;
    background-color: rgba(0,0,0,0.03);
    padding: 10px;
    border-radius: 4px;
}

/* Estilo para el mensaje de advertencia */
.warning-message {
    background-color: rgba(255, 152, 0, 0.15);
    border-left: 3px solid #FF9800;
    padding: 10px;
    border-radius: 4px;
    margin: 10px 0;
}

/* Estilos para botones de conversaci√≥n */
.conversation-button {
    background-color: var(--primary-color);
    color: white;
    border: none;
    border-radius: 8px;
    padding: 8px 12px;
    margin: 5px 0;
    cursor: pointer;
    width: 100%;
    font-weight: bold;
    text-align: center;
    transition: all 0.3s ease;
}

.conversation-button:hover {
    opacity: 0.9;
    box-shadow: 0 2px 5px rgba(0,0,0,0.2);
}

.conversation-button.new {
    background-color: var(--primary-color);
}

.conversation-button.clear {
    background-color: #f44336;
}

.conversation-button.cancel {
    background-color: #9e9e9e;
}

.conversation-button.confirm {
    background-color: #4CAF50;
}

/* Crear variables personalizadas basadas en el tema actual */
:root {
    --primary-color-light: rgba(var(--primary-color-rgb, 255, 75, 75), 0.1);
}

/* Mejoras para la legibilidad en tema oscuro */
[data-testid="stMarkdown"] {
    color: var(--text-color);
}

/* Estilo mejorado para botones de Streamlit */
button[data-testid="baseButton-secondary"] {
    background-color: var(--primary-color);
    color: white !important;
    border: none !important;
    border-radius: 8px;
    padding: 10px 15px;
    font-weight: bold;
    text-align: center;
    transition: all 0.2s ease;
    box-shadow: 0 1px 3px rgba(0,0,0,0.2);
}

button[data-testid="baseButton-secondary"]:hover {
    opacity: 0.9;
    box-shadow: 0 2px 5px rgba(0,0,0,0.3);
    transform: translateY(-1px);
}

/* Bot√≥n de limpiar chat */
.element-container:has(button:contains("üóëÔ∏è")) button[data-testid="baseButton-secondary"] {
    background-color: #f44336;
}

/* Bot√≥n de confirmaci√≥n */
.element-container:has(button:contains("‚úÖ")) button[data-testid="baseButton-secondary"] {
    background-color: #4CAF50;
}

/* Bot√≥n de cancelar */
.element-container:has(button:contains("‚ùå")) button[data-testid="baseButton-secondary"] {
    background-color: #9e9e9e;
}
</style>
""", unsafe_allow_html=True)


# Inicializar las variables de la sesi√≥n
if 'messages' not in st.session_state:
    st.session_state.messages = []
    
if 'pydantic_agent' not in st.session_state:
    st.session_state.pydantic_agent = None
    
if 'pydantic_history' not in st.session_state:
    st.session_state.pydantic_history = []
    
if 'memoria_activa' not in st.session_state:
    st.session_state.memoria_activa = False

# Variable para activar/desactivar herramientas
if 'usar_tavily' not in st.session_state:
    st.session_state.usar_tavily = True
    
# Inicializar variables de modelo y tipo
if 'modelo_seleccionado' not in st.session_state:
    st.session_state.modelo_seleccionado = "llama-3.3-70b-versatile"
    
if 'tipo_modelo' not in st.session_state:
    st.session_state.tipo_modelo = "Conversaci√≥n"
    
if 'temperatura' not in st.session_state:
    st.session_state.temperatura = 0.4
    
if 'max_tokens' not in st.session_state:
    st.session_state.max_tokens = 1024
    
if 'system_prompt' not in st.session_state:
    st.session_state.system_prompt = "Eres un asistente de inteligencia artificial √∫til, claro y conciso. Tu objetivo es ayudar al usuario proporcionando respuestas precisas y f√°ciles de entender. Siempre responde en espa√±ol, y si el usuario hace una pregunta ambigua, pide amablemente m√°s detalles."
    
if 'last_used_system_prompt' not in st.session_state:
    st.session_state.last_used_system_prompt = st.session_state.system_prompt
    
if 'confirmar_limpiar_chat' not in st.session_state:
    st.session_state.confirmar_limpiar_chat = False
    
if 'force_rerun' not in st.session_state:
    st.session_state.force_rerun = False
    
# Variables para el modo configuraci√≥n/chat
if 'pagina_actual' not in st.session_state:
    st.session_state.pagina_actual = 'configuracion'  # Valores: 'configuracion' o 'chat'
    
if 'config_guardada' not in st.session_state:
    st.session_state.config_guardada = False
    
if 'confirmar_cambio_config' not in st.session_state:
    st.session_state.confirmar_cambio_config = False
    
if 'configuraciones_guardadas' not in st.session_state:
    st.session_state.configuraciones_guardadas = {}
    
if 'config_actual' not in st.session_state:
    st.session_state.config_actual = {
        'modelo_seleccionado': st.session_state.modelo_seleccionado,
        'tipo_modelo': st.session_state.tipo_modelo,
        'system_prompt': st.session_state.system_prompt,
        'temperatura': 0.4,
        'max_tokens': 1024,
        'usar_tavily': True
    }

# Funci√≥n para limpiar la conversaci√≥n
def clear_conversation():
    # Guardar system prompt actual 
    current_system_prompt = st.session_state.system_prompt
    
    # Limpiar mensajes y reiniciar el historial
    st.session_state.messages = []
    if pydantic_available and st.session_state.memoria_activa:
        st.session_state.pydantic_history = []
    if 'last_request_params' in st.session_state:
        del st.session_state['last_request_params']
    
    # Restaurar el system prompt
    st.session_state.system_prompt = current_system_prompt
    
    # Reiniciar confirmaci√≥n de limpieza
    st.session_state.confirmar_limpiar_chat = False
    
    st.rerun()

# Funci√≥n para nueva conversaci√≥n
def new_conversation():
    # Limpiar mensajes y reiniciar el historial
    st.session_state.messages = []
    if pydantic_available and st.session_state.memoria_activa:
        st.session_state.pydantic_history = []
    if 'last_request_params' in st.session_state:
        del st.session_state['last_request_params']
    
    # Reiniciar el agente para que use el system prompt actual
    reset_pydantic_agent()
    
    st.rerun()

# Reiniciar agente pydantic cuando cambia la memoria
def reset_pydantic_agent():
    if "pydantic_agent" in st.session_state:
        st.session_state.pydantic_agent = None
    # Ya no eliminamos el historial al cambiar de modelo
    # if "pydantic_history" in st.session_state:
    #     st.session_state.pydantic_history = []
    
# Funciones para manejar cambios entre pantallas
def ir_a_configuracion():
    """Cambia a la pantalla de configuraci√≥n"""
    st.session_state.pagina_actual = 'configuracion'
    
def ir_a_chat():
    """Cambia a la pantalla de chat"""
    st.session_state.pagina_actual = 'chat'
    
def guardar_configuracion():
    """Guarda la configuraci√≥n actual y va a la pantalla de chat"""
    # Guardar la configuraci√≥n anterior para comparar
    config_anterior = st.session_state.config_actual.copy() if 'config_actual' in st.session_state else None
    cambio_modo = False
    
    # Verificar si se cambi√≥ el tipo de modelo
    if config_anterior and config_anterior['tipo_modelo'] != st.session_state.tipo_modelo:
        cambio_modo = True
    
    # Guardar la configuraci√≥n actual
    st.session_state.config_actual = {
        'modelo_seleccionado': st.session_state.modelo_seleccionado,
        'tipo_modelo': st.session_state.tipo_modelo,
        'system_prompt': st.session_state.system_prompt,
        'temperatura': st.session_state.temperatura,
        'max_tokens': st.session_state.max_tokens,
        'usar_tavily': st.session_state.usar_tavily
    }
    
    # Actualizar las variables globales para que se apliquen de inmediato
    global tipo_modelo, modelo_seleccionado, temperatura, max_tokens, razonamiento_formato
    tipo_modelo = st.session_state.tipo_modelo
    modelo_seleccionado = st.session_state.modelo_seleccionado
    temperatura = st.session_state.temperatura
    max_tokens = st.session_state.max_tokens
    
    # Establecer formato de razonamiento para modelos de razonamiento
    razonamiento_formato = None
    if tipo_modelo == "Razonamiento":
        razonamiento_formato = "raw"
        # Si cambiamos a modo razonamiento, forzar recarga completa 
        if cambio_modo:
            st.session_state.force_rerun = True
    
    # Marcar configuraci√≥n como guardada
    st.session_state.config_guardada = True
    
    # Reiniciar el agente para usar la nueva configuraci√≥n
    reset_pydantic_agent()
    
    # Cambiar a la pantalla de chat
    ir_a_chat()
    
def confirmar_cambio_configuracion():
    """Pide confirmaci√≥n para cambiar a configuraci√≥n si hay mensajes"""
    if len(st.session_state.messages) > 0:
        st.session_state.confirmar_cambio_config = True
    else:
        ir_a_configuracion()
    
# Definici√≥n de modelos disponibles
MODELOS = {
    "Conversaci√≥n": [
        "llama-3.3-70b-versatile",
        "qwen-2.5-32b",
        "qwen-2.5-coder-32b",
        "qwen-qwq-32b"
    ],
    "Audio a Texto": [
        "whisper-large-v3",
        "whisper-large-v3-turbo",
        "distil-whisper-large-v3-en"
    ],
    "Razonamiento": [
        "deepseek-r1-distill-llama-70b",
        "deepseek-r1-distill-qwen-32b"
    ]
}

# Funci√≥n para obtener la API key de Groq de forma segura
def get_groq_api_key():
    # Primero intentar obtenerla del archivo .env
    api_key = os.getenv("GROQ_API_KEY")
    
    # Si no est√° en el archivo .env, intentar obtenerla del entorno
    if not api_key:
        # Como √∫ltimo recurso, pedirla al usuario (solo en desarrollo)
        api_key = st.sidebar.text_input("Introduce tu API key de Groq:", type="password")
        if not api_key:
            st.sidebar.warning("Por favor, introduce tu API key de Groq para continuar.")
    
    return api_key

# Funci√≥n para obtener la API key de Tavily de forma segura
def get_tavily_api_key():
    # Intentar obtenerla del archivo .env
    api_key = os.getenv("TAVILY_API_KEY")
    
    # Si no est√° en el archivo .env, mostrar mensaje informativo
    if not api_key and tavily_available:
        st.sidebar.info("Para habilitar la b√∫squeda web, a√±ade TAVILY_API_KEY en tu archivo .env")
    
    return api_key

# Funci√≥n para buscar en internet con Tavily Search API
def buscar_en_internet(query: str, num_results: int = 5) -> str:
    """
    Realiza una b√∫squeda en internet utilizando Tavily Search API.
    
    Args:
        query (str): La consulta a buscar
        num_results (int): N√∫mero de resultados a devolver (default: 5)
        
    Returns:
        str: Resultados de la b√∫squeda formateados como texto
    """
    try:
        if not tavily_available:
            return "Error: La biblioteca tavily-python no est√° instalada. Inst√°lala con 'pip install tavily-python'."
        
        api_key = get_tavily_api_key()
        if not api_key:
            return "Error: No se encontr√≥ la API key de Tavily. Configura TAVILY_API_KEY en el archivo .env."
        
        # Inicializa el cliente de Tavily
        client = TavilyClient(api_key=api_key)
        
        # Realiza la b√∫squeda
        search_results = client.search(query=query, max_results=num_results, search_depth="advanced", include_answer=True)
        
        # Formatea los resultados como texto
        results_text = f"Resultados de b√∫squeda para: '{query}'\n\n"
        
        # A√±adir respuesta generada por Tavily si est√° disponible
        if "answer" in search_results and search_results["answer"]:
            results_text += f"RESPUESTA GENERADA:\n{search_results['answer']}\n\n"
        
        # A√±adir resultados de la b√∫squeda
        if "results" in search_results and search_results["results"]:
            results_text += "FUENTES:\n"
            for i, result in enumerate(search_results["results"][:num_results], 1):
                title = result.get("title", "Sin t√≠tulo")
                url = result.get("url", "Sin URL")
                content = result.get("content", "Sin contenido")
                score = result.get("score", 0)
                
                results_text += f"{i}. {title}\n   URL: {url}\n   Relevancia: {score:.2f}\n   {content[:200]}...\n\n"
        
        # Registrar la b√∫squeda para depuraci√≥n
        st.session_state["ultima_busqueda"] = {
            "query": query,
            "resultados": results_text
        }
        
        return results_text
    except Exception as e:
        error_msg = f"Error al realizar la b√∫squeda: {str(e)}"
        st.session_state["ultima_busqueda_error"] = error_msg
        return error_msg

# Funci√≥n para buscar en DuckDuckGo
def buscar_en_duckduckgo(query: str, num_results: int = 5) -> str:
    """
    Realiza una b√∫squeda en internet utilizando DuckDuckGo Search.
    
    Args:
        query (str): La consulta a buscar
        num_results (int): N√∫mero de resultados a devolver (default: 5)
        
    Returns:
        str: Resultados de la b√∫squeda formateados como texto
    """
    try:
        if not duckduckgo_available:
            return "Error: La biblioteca duckduckgo-search no est√° instalada. Inst√°lala con 'pip install duckduckgo-search'."
        
        # Importamos aqu√≠ para evitar problemas de importaci√≥n si no est√° disponible
        from duckduckgo_search import DDGS
        
        # Inicializa el cliente de DuckDuckGo
        ddgs = DDGS()
        
        # Realiza la b√∫squeda
        search_results = list(ddgs.text(query, max_results=num_results))
        
        # Formatea los resultados como texto
        results_text = f"Resultados de b√∫squeda en DuckDuckGo para: '{query}'\n\n"
        
        # A√±adir resultados de la b√∫squeda
        if search_results:
            for i, result in enumerate(search_results, 1):
                title = result.get("title", "Sin t√≠tulo")
                url = result.get("href", "Sin URL")
                content = result.get("body", "Sin contenido")
                
                results_text += f"{i}. {title}\n   URL: {url}\n   {content[:200]}...\n\n"
        else:
            results_text += "No se encontraron resultados para esta consulta.\n"
        
        # Registrar la b√∫squeda para depuraci√≥n
        st.session_state["ultima_busqueda_ddg"] = {
            "query": query,
            "resultados": results_text
        }
        
        return results_text
    except Exception as e:
        error_msg = f"Error al realizar la b√∫squeda en DuckDuckGo: {str(e)}"
        st.session_state["ultima_busqueda_ddg_error"] = error_msg
        return error_msg

# Configuraci√≥n del cliente de Groq
@st.cache_resource
def get_groq_client(api_key):
    if not api_key:
        return None
    return groq.Groq(api_key=api_key)

# Funci√≥n para inicializar o actualizar el agente PydanticAI
def setup_pydantic_agent(api_key, model_name):
    if not pydantic_available:
        st.error("La biblioteca pydantic-ai no est√° instalada. La funci√≥n de memoria no est√° disponible.")
        return None
        
    try:
        # Configurar el nombre del modelo para Groq
        agent_name = f"groq:{model_name}"
        
        # Configurar las variables de entorno para el agente
        os.environ["GROQ_API_KEY"] = api_key
        
        # Obtener la API key de Tavily
        tavily_api_key = get_tavily_api_key()
        
        # Obtener el system prompt personalizado del usuario
        base_system_prompt = st.session_state.system_prompt
        
        # Obtener la fecha y hora actuales
        now = datetime.now()
        date_time_str = now.strftime("%d/%m/%Y %H:%M:%S")
        
        # A√±adir informaci√≥n sobre fecha y hora actuales
        system_prompt = f"{base_system_prompt} La fecha y hora actuales son: {date_time_str}."
        
        # A√±adir informaci√≥n sobre la b√∫squeda web solo si est√° habilitada
        if st.session_state.usar_tavily and tavily_api_key and tavily_available:
            # A√±adir instrucci√≥n sobre el uso de la herramienta de b√∫squeda web
            if not "b√∫squeda web" in system_prompt.lower() and not "tavily" in system_prompt.lower():
                system_prompt += " Cuando el usuario solicite informaci√≥n actualizada o sobre eventos recientes, utiliza la herramienta de b√∫squeda web de Tavily para obtener y proporcionar informaci√≥n en tiempo real de fuentes confiables."
        
        # Guardar el system prompt realmente usado
        st.session_state.system_prompt_actual = system_prompt
        
        # Crear un nuevo agente con el system prompt actualizado
        agent = Agent(
            agent_name,
            system_prompt=system_prompt
        )
        
        # Registrar la herramienta de b√∫squeda web solo si est√° habilitada y disponible
        if st.session_state.usar_tavily and tavily_api_key and tavily_available:
            @agent.tool
            def search_web(ctx: RunContext[str], query: str, num_results: int = 5) -> str:
                """
                Busca informaci√≥n en internet utilizando Tavily Search.
                
                Args:
                    query (str): La consulta a buscar en internet
                    num_results (int, optional): N√∫mero de resultados a mostrar. Default: 5
                    
                Returns:
                    str: Resultados de la b√∫squeda formateados
                """
                resultado = buscar_en_internet(query, num_results)
                # Registrar el uso de la herramienta para verificaci√≥n
                if "herramientas_usadas" not in st.session_state:
                    st.session_state.herramientas_usadas = []
                st.session_state.herramientas_usadas.append({
                    "tool": "search_web", 
                    "query": query,
                    "resultado_corto": resultado[:100] + "..." if len(resultado) > 100 else resultado
                })
                return resultado
        
        return agent
    except Exception as e:
        st.error(f"Error al inicializar el agente PydanticAI: {str(e)}")
        return None

# Sidebar - Selecci√≥n de modelo
st.sidebar.title("Gesti√≥n de Conversaciones")

# Manejo de confirmaci√≥n de limpieza de chat
if st.session_state.confirmar_limpiar_chat:
    # Mostrar mensaje de confirmaci√≥n para limpiar chat
    st.sidebar.warning("‚ö†Ô∏è Se perder√° todo el historial de la conversaci√≥n. ¬øEst√°s seguro?")
    
    # Botones uno debajo del otro
    if st.sidebar.button("‚úÖ S√≠, limpiar chat", key="confirmar_limpiar_btn", use_container_width=True):
        clear_conversation()
    
    if st.sidebar.button("‚ùå Cancelar", key="cancelar_limpiar_btn", use_container_width=True):
        st.session_state.confirmar_limpiar_chat = False
        st.rerun()

# Manejo de confirmaci√≥n para cambio de configuraci√≥n
elif st.session_state.confirmar_cambio_config:
    st.sidebar.warning("‚ö†Ô∏è Cambiar a configuraci√≥n iniciar√° una nueva conversaci√≥n. Se perder√° el historial actual. ¬øEst√°s seguro?")
    
    if st.sidebar.button("‚úÖ S√≠, ir a configuraci√≥n", key="confirmar_config_btn", use_container_width=True):
        # Limpiar mensajes y reiniciar historial
        st.session_state.messages = []
        if pydantic_available and st.session_state.memoria_activa:
            st.session_state.pydantic_history = []
        st.session_state.confirmar_cambio_config = False
        ir_a_configuracion()
        st.rerun()
    
    if st.sidebar.button("‚ùå Cancelar", key="cancelar_config_btn", use_container_width=True):
        st.session_state.confirmar_cambio_config = False
        st.rerun()
        
# Si estamos en la p√°gina de chat, mostramos botones de gesti√≥n
elif st.session_state.pagina_actual == 'chat':
    # Definir las variables de sesi√≥n para control de eventos si no existen
    if 'nueva_conv_pendiente' not in st.session_state:
        st.session_state.nueva_conv_pendiente = False
    
    # Botones uno debajo del otro
    if st.sidebar.button("‚öôÔ∏è Cambiar configuraci√≥n", key="cambiar_config_btn", use_container_width=True):
        confirmar_cambio_configuracion()
        st.rerun()
    
    if st.sidebar.button("üîÑ Nueva conversaci√≥n", key="nueva_conv_btn", use_container_width=True):
        # Evitar st.rerun() en callbacks
        st.session_state.nueva_conv_pendiente = True
    
    st.sidebar.button("üóëÔ∏è Limpiar chat", key="limpiar_chat_btn", use_container_width=True, 
             on_click=lambda: setattr(st.session_state, 'confirmar_limpiar_chat', True))
            
    # Procesar acci√≥n pendiente si existe
    if st.session_state.nueva_conv_pendiente:
        st.session_state.nueva_conv_pendiente = False
        new_conversation()

# Control de flujo principal de la aplicaci√≥n
if st.session_state.pagina_actual == 'configuracion':
    # Mostrar la p√°gina de configuraci√≥n
    st.title("Configuraci√≥n del Asistente IA")
    st.markdown("### Configura tu asistente antes de comenzar la conversaci√≥n")
    
    # Secci√≥n de selecci√≥n de modelo
    st.subheader("1. Selecci√≥n de Modelo")
    
    # Selecci√≥n de tipo de modelo (ahora como dropdown)
    tipo_modelo = st.selectbox(
        "Tipo de modelo:",
        list(MODELOS.keys()),
        index=list(MODELOS.keys()).index(st.session_state.tipo_modelo),
        key="tipo_modelo_select_config"
    )
    
    # Actualizar el tipo de modelo en la sesi√≥n
    if st.session_state.tipo_modelo != tipo_modelo:
        st.session_state.tipo_modelo = tipo_modelo
        # Establecer un modelo predeterminado para este tipo
        st.session_state.modelo_seleccionado = MODELOS[tipo_modelo][0]
        st.rerun()
    
    # Selecci√≥n del modelo espec√≠fico
    modelo_seleccionado = st.selectbox(
        "Modelo:",
        MODELOS[tipo_modelo],
        index=(MODELOS[tipo_modelo].index(st.session_state.modelo_seleccionado) 
               if st.session_state.modelo_seleccionado in MODELOS[tipo_modelo] 
               else 0)
    )
    
    # Actualizar el modelo seleccionado en la sesi√≥n
    if st.session_state.modelo_seleccionado != modelo_seleccionado:
        st.session_state.modelo_seleccionado = modelo_seleccionado
    
    # Secci√≥n de par√°metros
    st.subheader("2. Par√°metros del Modelo")
    
    # Ajustes de temperatura y tokens
    col1, col2 = st.columns(2)
    with col1:
        temperatura = st.slider("Temperatura:", min_value=0.0, max_value=1.0, value=0.4, step=0.1)
        st.session_state.temperatura = temperatura
    
    with col2:
        max_tokens = st.slider("Tokens m√°ximos:", min_value=256, max_value=4096, value=1024, step=128)
        st.session_state.max_tokens = max_tokens
    
    # Configuraci√≥n del System Prompt
    st.subheader("3. Instrucciones para el Asistente (System Prompt)")
    
    # √Årea de texto para editar el system prompt
    system_prompt = st.text_area(
        "Instrucciones personalizadas:",
        value=st.session_state.system_prompt,
        height=150,
        help="Define c√≥mo debe comportarse el asistente en este chat."
    )
    st.session_state.system_prompt = system_prompt
    
    # Secci√≥n de herramientas
    st.subheader("4. Herramientas")
    
    # Informaci√≥n sobre Tavily Search
    tavily_status = "‚úÖ Configurada" if get_tavily_api_key() else "‚ùå No configurada (a√±ade TAVILY_API_KEY en .env)"
    
    # Checkbox para activar/desactivar Tavily
    st.session_state.usar_tavily = st.checkbox(
        f"Habilitar b√∫squeda web con Tavily ({tavily_status})",
        value=st.session_state.usar_tavily,
        help="Permite que el asistente busque informaci√≥n actualizada en internet"
    )
    
    if not get_tavily_api_key() and st.session_state.usar_tavily:
        st.info("Para usar la b√∫squeda web, a√±ade TAVILY_API_KEY en tu archivo .env")
    
    # Bot√≥n para restablecer valores predeterminados
    if st.button("üîÑ Restablecer valores predeterminados", use_container_width=True):
        # Establecer valores predeterminados
        st.session_state.tipo_modelo = "Conversaci√≥n"
        st.session_state.modelo_seleccionado = "llama-3.3-70b-versatile"
        st.session_state.temperatura = 0.9
        st.session_state.max_tokens = 1024
        st.session_state.system_prompt = "Eres un asistente de inteligencia artificial √∫til, claro y conciso. Tu objetivo es ayudar al usuario proporcionando respuestas precisas y f√°ciles de entender. Siempre responde en espa√±ol, y si el usuario hace una pregunta ambigua, pide amablemente m√°s detalles."
        st.session_state.usar_tavily = True
        st.rerun()
    
    # Bot√≥n para guardar la configuraci√≥n e ir al chat
    if st.button("üíæ Guardar configuraci√≥n e iniciar chat", use_container_width=True, type="primary"):
        guardar_configuracion()
        
        # Si necesitamos forzar una recarga
        if "force_rerun" in st.session_state and st.session_state.force_rerun:
            st.session_state.force_rerun = False
            st.rerun()
        else:
            st.rerun()
        
elif st.session_state.pagina_actual == 'chat':
    # Verificar si hay configuraci√≥n guardada
    if not st.session_state.config_guardada:
        st.warning("Por favor, configura tu asistente antes de comenzar la conversaci√≥n.")
        if st.button("Ir a configuraci√≥n"):
            ir_a_configuracion()
            st.rerun()
    else:
        # Mostrar la p√°gina de chat con la configuraci√≥n actual
        st.title("Asistente IA con Groq ü§ñ")
        
        # Actualizar el sidebar con informaci√≥n
        st.sidebar.markdown("---")
        st.sidebar.subheader("Configuraci√≥n Actual")
        st.sidebar.info(f"**Modelo:** {st.session_state.config_actual['tipo_modelo']} - {st.session_state.config_actual['modelo_seleccionado']}")
        st.sidebar.caption(f"Temperatura: {st.session_state.config_actual['temperatura']}")
        st.sidebar.caption(f"Tokens m√°ximos: {st.session_state.config_actual['max_tokens']}")
        
        # Agregar el system prompt en un acorde√≥n en el sidebar con mejor estilo
        with st.sidebar.expander("Ver System Prompt actual", expanded=False):
            # Mostrar el system prompt real que se est√° utilizando
            prompt_to_display = st.session_state.system_prompt_actual if 'system_prompt_actual' in st.session_state else st.session_state.config_actual['system_prompt']
            
            st.markdown(
                f"""<div style="background-color: var(--secondary-background-color); 
                           color: var(--text-color); 
                           font-family: monospace; 
                           padding: 10px; 
                           border-radius: 5px; 
                           font-size: 12px; 
                           word-wrap: break-word; 
                           white-space: pre-wrap;
                           overflow-wrap: break-word;
                           border: 1px solid rgba(128, 128, 128, 0.2);">{prompt_to_display}</div>""", 
                unsafe_allow_html=True
            )
        
        # Mostrar estado de herramientas en el sidebar
        st.sidebar.caption(f"B√∫squeda web: {'Activada' if st.session_state.config_actual.get('usar_tavily', False) else 'Desactivada'}")
        
        # Mantenemos esta informaci√≥n en el sidebar
        st.sidebar.markdown("---")

# Mantenemos esta informaci√≥n en el sidebar
st.sidebar.markdown("---")

# Obtener API key y cliente
api_key = get_groq_api_key()
client = get_groq_client(api_key)

# Si estamos en la p√°gina de chat, inicializar el agente con la configuraci√≥n guardada
if st.session_state.pagina_actual == 'chat' and st.session_state.config_guardada:
    # Establecer par√°metros actuales para el chat
    tipo_modelo = st.session_state.config_actual['tipo_modelo']
    modelo_seleccionado = st.session_state.config_actual['modelo_seleccionado']
    temperatura = st.session_state.config_actual['temperatura']
    max_tokens = st.session_state.config_actual['max_tokens']
    
    # Asegurar que los valores en la sesi√≥n est√°n sincronizados
    st.session_state.tipo_modelo = tipo_modelo
    st.session_state.modelo_seleccionado = modelo_seleccionado
    
    # Establecer formato de razonamiento para modelos de razonamiento
    razonamiento_formato = None
    if tipo_modelo == "Razonamiento":
        razonamiento_formato = "raw"
    
    # Activar la memoria siempre si pydantic est√° disponible
    if pydantic_available and tipo_modelo in ["Conversaci√≥n", "Razonamiento"]:
        # Siempre activar la memoria
        st.session_state.memoria_activa = True
        
        # Inicializar el agente PydanticAI si no existe
        if api_key and (st.session_state.pydantic_agent is None):
            st.session_state.pydantic_agent = setup_pydantic_agent(api_key, modelo_seleccionado)
            if st.session_state.pydantic_agent is None:
                st.session_state.memoria_activa = False

# Subida de archivos para modelos de audio
uploaded_file = None
if st.session_state.pagina_actual == 'chat' and st.session_state.config_guardada:
    if tipo_modelo == "Audio a Texto":
        uploaded_file = st.file_uploader("Sube un archivo de audio (mp3, wav, m4a, ogg)", type=["mp3", "wav", "m4a", "ogg"])
        if uploaded_file is not None:
            # Determinar el formato correcto para reproducir el audio
            file_type = uploaded_file.type.split('/')[1] if '/' in uploaded_file.type else 'audio/ogg'
            st.audio(uploaded_file, format=file_type)

# Procesar archivo de audio
def process_audio_file(file, model):
    try:
        # Leer el archivo y codificarlo en base64
        file_bytes = file.getvalue()
        
        # Determinar el tipo de archivo para casos donde no se detecta correctamente
        file_type = file.type
        if not file_type or '/' not in file_type:
            # Inferir tipo basado en la extensi√≥n
            if file.name.lower().endswith('.ogg'):
                file_type = 'audio/ogg'
            elif file.name.lower().endswith('.mp3'):
                file_type = 'audio/mpeg'
            elif file.name.lower().endswith('.wav'):
                file_type = 'audio/wav'
            elif file.name.lower().endswith('.m4a'):
                file_type = 'audio/m4a'
            else:
                file_type = 'audio/ogg'  # Default para archivos de WhatsApp
        
        # Crear la solicitud a la API
        transcription = client.audio.transcriptions.create(
            model=model,
            file=(file.name, file_bytes)
        )
        
        return transcription.text
    except Exception as e:
        return f"Error al procesar el audio: {str(e)}"

# Funci√≥n para limpiar y procesar etiquetas <think> en un mensaje
def procesar_mensaje_razonamiento(contenido):
    """
    Procesa un mensaje para detectar y formatear etiquetas <think>
    Retorna el contenido procesado
    """
    # Limpiar agresivamente cualquier etiqueta HTML incorrecta
    html_patterns = [
        "</div>", 
        '<div class="chat-text">', 
        '<div class=', 
        '</div>', 
        '<span', 
        '</span>'
    ]
    
    # Crear una copia para procesar
    clean_content = contenido
    
    # Verificar si el contenido ya tiene etiquetas HTML incorrectas
    has_html_tags = any(pattern in clean_content for pattern in html_patterns)
    
    # Limpiar todas las etiquetas HTML posibles
    if has_html_tags:
        for pattern in html_patterns:
            clean_content = clean_content.replace(pattern, '')
        clean_content = clean_content.strip()
    
    # Verificar si hay etiquetas <think>
    has_thinking_tags = False
    thinking_content = ""
    answer_content = ""
    
    # Probar formato sin escapar
    if "<think>" in clean_content and "</think>" in clean_content:
        has_thinking_tags = True
        thinking_start = clean_content.find("<think>")
        thinking_end = clean_content.find("</think>")
        
        thinking_content = clean_content[thinking_start + 7:thinking_end].strip()
        answer_content = clean_content[thinking_end + 8:].strip()
        
        # Preservar el formato para visualizaci√≥n posterior
        return clean_content
        
    return clean_content

# Funci√≥n para enviar mensajes a Groq y obtener respuestas con streaming
def get_response_streaming(messages: List[Dict[str, str]], placeholder, razonamiento_formato=None, tipo_modelo_actual=None):
    try:
        if not client:
            return "Por favor, configura tu API key de Groq en un archivo .env"
        
        # Usar el tipo de modelo pasado como par√°metro, o el global si no se proporciona
        tipo_modelo_actual = tipo_modelo_actual or tipo_modelo
        
        # Preparar para streaming
        response_text = ""  # Para almacenar el texto puro sin HTML
        
        # Actualizar el primer mensaje (system) con la fecha y hora actuales
        from datetime import datetime
        now = datetime.now()
        date_time_str = now.strftime("%d/%m/%Y %H:%M:%S")
        
        # Asegurarse de que hay un mensaje de sistema
        if messages and messages[0]['role'] == 'system':
            # A√±adir informaci√≥n de fecha y hora al mensaje del sistema
            base_system_content = messages[0]['content']
            messages[0]['content'] = f"{base_system_content} La fecha y hora actuales son: {date_time_str}."
        
        # Registrar valores para verificaci√≥n
        st.session_state['last_request_params'] = {
            'temperatura': temperatura,
            'max_tokens': max_tokens,
            'modelo': modelo_seleccionado
        }
        
        # Configurar los par√°metros base
        params = {
            "model": modelo_seleccionado,
            "messages": messages,
            "temperature": temperatura,
            "max_tokens": max_tokens,
            "top_p": 1,
            "stream": True,
            "stop": None,
        }
        
        # A√±adir el formato de razonamiento si es un modelo de razonamiento
        if tipo_modelo_actual == "Razonamiento" and razonamiento_formato:
            params["reasoning_format"] = razonamiento_formato
        
        # Crear la solicitud de chat
        completion = client.chat.completions.create(**params)
        
        # Procesar la respuesta en streaming
        for chunk in completion:
            content = chunk.choices[0].delta.content or ""
            if content:
                # A√±adir al texto de respuesta puro (para almacenar)
                response_text += content
                
                # Escapar caracteres especiales HTML para la visualizaci√≥n
                safe_response = response_text.replace("<", "&lt;").replace(">", "&gt;")
                
                # Para modelos de razonamiento con formato "raw", formatear la salida
                if tipo_modelo_actual == "Razonamiento" and razonamiento_formato == "raw":
                    # Comprobar si hay etiquetas think incluso cuando no estamos en modelo de razonamiento
                    # Buscar el patr√≥n <think> y </think> en cualquier tipo de modelo
                    thinking_start = -1
                    thinking_end = -1
                    has_thinking_tags = False
                    thinking_content = ""
                    answer_content = ""
                    
                    # Buscar las etiquetas sin escapar en el texto original
                    if "<think>" in response_text and "</think>" in response_text:
                        has_thinking_tags = True
                        thinking_start = response_text.find("<think>")
                        thinking_end = response_text.find("</think>")
                        
                        # Extraer pensamiento y respuesta del texto original sin escapar
                        thinking_content = response_text[thinking_start + 7:thinking_end].strip()
                        answer_content = response_text[thinking_end + 8:].strip()
                    # Buscar las etiquetas escapadas en la respuesta ya procesada
                    elif "&lt;think&gt;" in safe_response and "&lt;/think&gt;" in safe_response:
                        has_thinking_tags = True
                        thinking_start = safe_response.find("&lt;think&gt;")
                        thinking_end = safe_response.find("&lt;/think&gt;")
                        
                        # Extraer contenido
                        thinking_content = safe_response[thinking_start + 12:thinking_end].strip()
                        answer_content = safe_response[thinking_end + 13:].strip()
                    
                    # Si se encontraron etiquetas de pensamiento, mostrar razonamiento y respuesta
                    if has_thinking_tags:
                        # Escapar para visualizaci√≥n si no estaba ya escapado
                        if "<" in thinking_content or ">" in thinking_content:
                            safe_thinking = thinking_content.replace("<", "&lt;").replace(">", "&gt;")
                        else:
                            safe_thinking = thinking_content
                        
                        if "<" in answer_content or ">" in answer_content:
                            safe_answer = answer_content.replace("<", "&lt;").replace(">", "&gt;")
                        else:
                            safe_answer = answer_content
                        
                        # Si el answer_content est√° vac√≠o, a√±adir un mensaje predeterminado
                        if not safe_answer:
                            safe_answer = "El modelo proporcion√≥ s√≥lo su razonamiento."
                        
                        placeholder.markdown(
                            f"""
                            <div class="chat-container assistant-message">
                                <div class="assistant-header">
                                    Asistente:
                                </div>
                                <div class="reasoning-section">
                                    <div class="thinking-title">Razonamiento:</div>
                                    <div class="reasoning-content">{safe_thinking}</div>
                                </div>
                                <div class="chat-text">{safe_answer}</div>
                            </div>
                            """, 
                            unsafe_allow_html=True
                        )
                        # Salimos de esta rama, ya procesamos el mensaje
                        return response_text
                    else:
                        # No hay etiquetas de razonamiento, mostrar el mensaje normal
                        placeholder.markdown(
                            f"""
                            <div class="chat-container assistant-message">
                                <div class="assistant-header">
                                    Asistente:
                                </div>
                                <div class="chat-text">{safe_response}</div>
                            </div>
                            """, 
                            unsafe_allow_html=True
                        )
                else:
                    # Comprobar si hay etiquetas think incluso cuando no estamos en modelo de razonamiento
                    thinking_start = -1
                    thinking_end = -1
                    has_thinking_tags = False
                    thinking_content = ""
                    answer_content = ""
                    
                    # Buscar las etiquetas sin escapar en el texto original
                    if "<think>" in response_text and "</think>" in response_text:
                        has_thinking_tags = True
                        thinking_start = response_text.find("<think>")
                        thinking_end = response_text.find("</think>")
                        
                        # Extraer pensamiento y respuesta del texto original sin escapar
                        thinking_content = response_text[thinking_start + 7:thinking_end].strip()
                        answer_content = response_text[thinking_end + 8:].strip()
                    # Buscar las etiquetas escapadas en la respuesta ya procesada
                    elif "&lt;think&gt;" in safe_response and "&lt;/think&gt;" in safe_response:
                        has_thinking_tags = True
                        thinking_start = safe_response.find("&lt;think&gt;")
                        thinking_end = safe_response.find("&lt;/think&gt;")
                        
                        # Extraer contenido
                        thinking_content = safe_response[thinking_start + 12:thinking_end].strip()
                        answer_content = safe_response[thinking_end + 13:].strip()
                    
                    # Si se encontraron etiquetas de pensamiento, mostrar razonamiento y respuesta
                    if has_thinking_tags:
                        # Escapar para visualizaci√≥n si no estaba ya escapado
                        if "<" in thinking_content or ">" in thinking_content:
                            safe_thinking = thinking_content.replace("<", "&lt;").replace(">", "&gt;")
                        else:
                            safe_thinking = thinking_content
                        
                        if "<" in answer_content or ">" in answer_content:
                            safe_answer = answer_content.replace("<", "&lt;").replace(">", "&gt;")
                        else:
                            safe_answer = answer_content
                        
                        # Si el answer_content est√° vac√≠o, a√±adir un mensaje predeterminado
                        if not safe_answer:
                            safe_answer = "El modelo proporcion√≥ s√≥lo su razonamiento."
                        
                        placeholder.markdown(
                            f"""
                            <div class="chat-container assistant-message">
                                <div class="assistant-header">
                                    Asistente:
                                </div>
                                <div class="reasoning-section">
                                    <div class="thinking-title">Razonamiento:</div>
                                    <div class="reasoning-content">{safe_thinking}</div>
                                </div>
                                <div class="chat-text">{safe_answer}</div>
                            </div>
                            """, 
                            unsafe_allow_html=True
                        )
                        return response_text
                    else:
                        # No hay etiquetas de razonamiento, mostrar el mensaje normal
                        placeholder.markdown(
                            f"""
                            <div class="chat-container assistant-message">
                                <div class="assistant-header">
                                    Asistente:
                                </div>
                                <div class="chat-text">{safe_response}</div>
                            </div>
                            """, 
                            unsafe_allow_html=True
                        )
        
        # Limpiar posibles etiquetas HTML en la respuesta antes de devolverla
        html_patterns = [
            "</div>", 
            '<div class="chat-text">', 
            '<div class=', 
            '</div>', 
            '<span', 
            '</span>'
        ]
        
        # Verificar si hay etiquetas HTML incorrectas
        clean_response_text = response_text
        for pattern in html_patterns:
            if pattern in clean_response_text:
                clean_response_text = clean_response_text.replace(pattern, '')
                
        # Devolver el texto limpio
        return clean_response_text.strip()
    except Exception as e:
        return f"Error al comunicarse con Groq: {str(e)}"

# Funci√≥n para obtener respuesta usando PydanticAI (con memoria)
def get_response_with_memory(user_prompt):
    try:
        # Verificar que pydantic est√° disponible
        if not pydantic_available:
            return "La funci√≥n de memoria requiere la biblioteca 'pydantic-ai'"
        
        # Actualizar la fecha y hora en el system prompt
        from datetime import datetime
        now = datetime.now()
        date_time_str = now.strftime("%d/%m/%Y %H:%M:%S")
        
        # Verificar si el system prompt ha cambiado desde el √∫ltimo uso
        system_prompt_changed = st.session_state.system_prompt != st.session_state.last_used_system_prompt
        
        # Verificar que el agente existe y asegurarse de que est√© usando el system prompt actual
        if st.session_state.pydantic_agent is None or system_prompt_changed:
            st.session_state.pydantic_agent = setup_pydantic_agent(api_key, modelo_seleccionado)
            # Actualizar el √∫ltimo system prompt utilizado
            st.session_state.last_used_system_prompt = st.session_state.system_prompt
            
            if st.session_state.pydantic_agent is None:
                return "No se pudo inicializar el agente de memoria"
        else:
            # Si el agente ya existe, solo actualizamos la fecha y hora
            # Obtener el system prompt base (sin fecha/hora ni instrucciones de Tavily)
            base_system_prompt = st.session_state.system_prompt
            
            # Reconstruir el system prompt con la fecha y hora actualizadas
            system_prompt = f"{base_system_prompt} La fecha y hora actuales son: {date_time_str}."
            
            # A√±adir informaci√≥n sobre la b√∫squeda web si est√° habilitada
            if st.session_state.usar_tavily and get_tavily_api_key() and tavily_available:
                if not "b√∫squeda web" in system_prompt.lower() and not "tavily" in system_prompt.lower():
                    system_prompt += " Cuando el usuario solicite informaci√≥n actualizada o sobre eventos recientes, utiliza la herramienta de b√∫squeda web de Tavily para obtener y proporcionar informaci√≥n en tiempo real de fuentes confiables."
            
            # Actualizar el system prompt del agente
            st.session_state.pydantic_agent._system_prompt = system_prompt
            
            # Guardar el system prompt realmente usado
            st.session_state.system_prompt_actual = system_prompt
        
        # Ejecutar el agente con la historia de mensajes
        try:
            # Placeholder para mostrar informaci√≥n sobre herramientas
            tools_placeholder = st.empty()
            tools_placeholder.markdown("‚è≥ Procesando tu consulta...")
            
            # Limpiar registro de herramientas utilizadas para esta consulta
            if "herramientas_usadas" in st.session_state:
                st.session_state.herramientas_usadas = []
            
            # Ejecutar el agente con la historia de mensajes y mostrar el proceso
            st.session_state.debug_info = []
            
            # Configurar el callback para visualizar el proceso
            def tool_callback(event_type, event_data):
                if event_type == "tool_start":
                    tool_info = f"Usando herramienta: {event_data.get('name', 'desconocida')}, Argumentos: {event_data.get('input')}"
                    st.session_state.debug_info.append(tool_info)
                    # Actualizar el placeholder para mostrar la herramienta en uso
                    tools_placeholder.markdown(f"‚öôÔ∏è {tool_info}...")
                elif event_type == "tool_end":
                    if "result" in event_data:
                        result_summary = event_data["result"][:500] + "..." if len(event_data["result"]) > 500 else event_data["result"]
                        tool_result = f"Resultado: {result_summary}"
                        st.session_state.debug_info.append(tool_result)
            
            # Ejecutar el agente con callback
            try:
                result = st.session_state.pydantic_agent.run_sync(
                    user_prompt, 
                    message_history=st.session_state.pydantic_history,
                    callbacks=[tool_callback]
                )
            except TypeError:
                # Si el callback causa problemas, ejecutar sin √©l
                result = st.session_state.pydantic_agent.run_sync(
                    user_prompt, 
                    message_history=st.session_state.pydantic_history
                )
            
            # Actualizar la historia de mensajes
            st.session_state.pydantic_history = result.all_messages()
            
            # Verificar si se utilizaron herramientas
            response_data = result.data
            
            # Verificar el registro de herramientas utilizadas
            if "herramientas_usadas" in st.session_state and st.session_state.herramientas_usadas:
                # Construir un razonamiento con los resultados de Tavily
                thinking_content = "Consultando fuentes en tiempo real...\n\n"
                
                for tool_use in st.session_state.herramientas_usadas:
                    if tool_use["tool"] == "search_web":
                        thinking_content += f"B√∫squeda web para: '{tool_use['query']}'\n\n"
                        
                        # Obtener los resultados completos de la √∫ltima b√∫squeda
                        if "ultima_busqueda" in st.session_state:
                            thinking_content += st.session_state["ultima_busqueda"]["resultados"]
                
                # Formatear la respuesta con el formato de pensamiento para que se muestre en la UI
                # Verificar si la respuesta ya contiene etiquetas de funci√≥n
                if "<function=" in response_data:
                    # Eliminar las etiquetas de funci√≥n que est√°n apareciendo en la UI
                    response_data = response_data.replace("<function=", "").replace("</function>", "")
                    # Si hay informaci√≥n JSON, intentamos limpiarla tambi√©n
                    import re
                    response_data = re.sub(r'\{.*?\}', '', response_data)
                
                response_data = f"<think>{thinking_content}</think>{response_data}"
            
            # Limpiar el placeholder de herramientas
            tools_placeholder.empty()
            
            # Devolver los datos procesados
            return response_data
            
        except Exception as inner_e:
            # Si hay un error espec√≠fico con la ejecuci√≥n
            st.warning(f"Ajustando configuraci√≥n: {str(inner_e)}")
            result = st.session_state.pydantic_agent.run_sync(user_prompt)
            
            # Actualizar la historia de mensajes
            st.session_state.pydantic_history = result.all_messages()
            return result.data
            
    except Exception as e:
        return f"Error al utilizar la memoria: {str(e)}"

# Mostrar historial de mensajes
if st.session_state.pagina_actual == 'chat' and st.session_state.config_guardada:
    # Utilizar tipo_modelo de la configuraci√≥n actual
    current_tipo_modelo = st.session_state.config_actual['tipo_modelo']
    
    for message in st.session_state.messages:
        role = message["role"]
        content = message["content"]
        
        if role == "user":
            # Escapar caracteres especiales HTML en el mensaje del usuario
            safe_content = content.replace("<", "&lt;").replace(">", "&gt;")
            
            st.markdown(f"""
            <div class="chat-container user-message">
                <div class="user-header">T√∫:</div>
                <div class="chat-text">{safe_content}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            # Limpiar agresivamente cualquier etiqueta HTML incorrecta
            # Patrones comunes de etiquetas que podr√≠an aparecer incorrectamente
            html_patterns = [
                "</div>", 
                '<div class="chat-text">', 
                '<div class=', 
                '</div>', 
                '<span', 
                '</span>'
            ]
            
            # Crear una copia para procesar
            clean_content = content
            
            # Verificar si el contenido ya tiene etiquetas HTML incorrectas
            has_html_tags = any(pattern in clean_content for pattern in html_patterns)
            
            # Limpiar todas las etiquetas HTML posibles
            if has_html_tags:
                for pattern in html_patterns:
                    clean_content = clean_content.replace(pattern, '')
                clean_content = clean_content.strip()
            
            # Escapar caracteres especiales HTML en el mensaje limpio
            safe_content = clean_content.replace("<", "&lt;").replace(">", "&gt;")
            
            # Siempre buscar etiquetas <think> independientemente del tipo de modelo
            # Buscar con y sin escape en HTML
            thinking_start = -1
            thinking_end = -1
            has_thinking_tags = False
            thinking_content = ""
            answer_content = ""
            
            # Probar formato sin escapar en el contenido limpio pero sin escapar
            if "<think>" in clean_content and "</think>" in clean_content:
                has_thinking_tags = True
                thinking_start = clean_content.find("<think>")
                thinking_end = clean_content.find("</think>")
                
                # Extraer contenido
                thinking_content = clean_content[thinking_start + 7:thinking_end].strip()
                answer_content = clean_content[thinking_end + 8:].strip()
            # Probar formato escapado como respaldo
            elif "&lt;think&gt;" in safe_content and "&lt;/think&gt;" in safe_content:
                has_thinking_tags = True
                thinking_start = safe_content.find("&lt;think&gt;")
                thinking_end = safe_content.find("&lt;/think&gt;")
                
                # Extraer contenido
                thinking_content = safe_content[thinking_start + 12:thinking_end].strip()
                answer_content = safe_content[thinking_end + 13:].strip()
            
            # Si se encontraron etiquetas de pensamiento, mostrar razonamiento y respuesta
            if has_thinking_tags:
                # Escapar para visualizaci√≥n si no estaba ya escapado
                if "<" in thinking_content or ">" in thinking_content:
                    safe_thinking = thinking_content.replace("<", "&lt;").replace(">", "&gt;")
                else:
                    safe_thinking = thinking_content
                    
                if "<" in answer_content or ">" in answer_content:
                    safe_answer = answer_content.replace("<", "&lt;").replace(">", "&gt;")
                else:
                    safe_answer = answer_content
                
                # Si el answer_content est√° vac√≠o, a√±adir un mensaje predeterminado
                if not safe_answer:
                    safe_answer = "El modelo proporcion√≥ s√≥lo su razonamiento."
                
                st.markdown(f"""
                <div class="chat-container assistant-message">
                    <div class="assistant-header">
                        Asistente:
                    </div>
                    <div class="reasoning-section">
                        <div class="thinking-title">Razonamiento:</div>
                        <div class="reasoning-content">{safe_thinking}</div>
                    </div>
                    <div class="chat-text">{safe_answer}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                # No hay etiquetas de razonamiento, mostrar el mensaje normal
                st.markdown(f"""
                <div class="chat-container assistant-message">
                    <div class="assistant-header">
                        Asistente:
                    </div>
                    <div class="chat-text">{safe_content}</div>
                </div>
                """, unsafe_allow_html=True)

# Input del usuario o procesamiento de audio
if st.session_state.pagina_actual == 'chat' and st.session_state.config_guardada:
    # Utilizar tipo_modelo de la configuraci√≥n actual
    current_tipo_modelo = st.session_state.config_actual['tipo_modelo']
    
    if current_tipo_modelo == "Audio a Texto" and uploaded_file is not None:
        if st.button("Transcribir Audio"):
            with st.spinner("Transcribiendo audio..."):
                transcription = process_audio_file(uploaded_file, modelo_seleccionado)
                
                # Agregar mensaje del usuario al historial
                archivo_mensaje = f"[Audio: {uploaded_file.name}]"
                st.session_state.messages.append({"role": "user", "content": archivo_mensaje})
                
                # Mostrar el mensaje del usuario
                st.markdown(f"""
                <div class="chat-container user-message">
                    <div class="user-header">T√∫:</div>
                    <div class="chat-text">{archivo_mensaje}</div>
                </div>
                """, unsafe_allow_html=True)
                
                # Crear un placeholder para la respuesta
                response_placeholder = st.empty()
                
                # Mostrar la transcripci√≥n
                response_placeholder.markdown(f"""
                <div class="chat-container assistant-message">
                    <div class="assistant-header">Transcripci√≥n:</div>
                    <div class="chat-text">{transcription}</div>
                </div>
                """, unsafe_allow_html=True)
                
                # Guardar la respuesta completa en el historial
                st.session_state.messages.append({"role": "assistant", "content": transcription})
                
                # Limpiar el archivo subido
                uploaded_file = None
                st.rerun()
    else:
        # Input de texto normal para otros tipos de modelos
        prompt = st.chat_input("Escribe tu mensaje aqu√≠...")
        
        # Procesar input del usuario
        if prompt and api_key:
            # Agregar mensaje del usuario al historial
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # Mostrar el mensaje del usuario
            st.markdown(f"""
            <div class="chat-container user-message">
                <div class="user-header">T√∫:</div>
                <div class="chat-text">{prompt}</div>
            </div>
            """, unsafe_allow_html=True)
            
            # Crear un placeholder para la respuesta en streaming
            response_placeholder = st.empty()
            
            # Usar pydantic s√≥lo si la memoria est√° activada manualmente por el usuario
            if pydantic_available and st.session_state.memoria_activa and current_tipo_modelo in ["Conversaci√≥n", "Razonamiento"]:
                # Obtener respuesta con memoria usando PydanticAI
                with st.spinner("Pensando..."):
                    response = get_response_with_memory(prompt)
                    
                    # Escapar caracteres especiales HTML en la respuesta
                    safe_response = response.replace("<", "&lt;").replace(">", "&gt;")
                    
                    # Formatear la respuesta
                    formatted_response = f"""
                    <div class="chat-container assistant-message">
                        <div class="assistant-header">
                            Asistente:
                        </div>
                        <div class="chat-text">{safe_response}</div>
                    </div>
                    """
                    
                    # Mostrar la respuesta
                    response_placeholder.markdown(formatted_response, unsafe_allow_html=True)
                    
                    # Procesar el mensaje para asegurar un formato correcto de etiquetas <think>
                    processed_response = procesar_mensaje_razonamiento(response)
                    
                    # Guardar la respuesta procesada en el historial
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": processed_response
                    })
            else:
                # Mostrar mensaje si pydantic no est√° disponible pero se intenta usar memoria
                if current_tipo_modelo in ["Conversaci√≥n", "Razonamiento"] and st.session_state.memoria_activa and not pydantic_available:
                    st.warning("Se recomienda instalar pydantic-ai para una mejor experiencia con modelos de conversaci√≥n y razonamiento.")
                
                # Preparar los mensajes para la API (sin memoria)
                messages_for_api = []
                
                # A√±adir sistema como primer mensaje con el system prompt actualizado
                messages_for_api.append({
                    "role": "system", 
                    "content": st.session_state.system_prompt  # Usar el system prompt actualizado
                })
                
                # A√±adir √∫ltimo mensaje del usuario
                last_user_msg = {"role": "user", "content": prompt}
                messages_for_api.append(last_user_msg)
                
                # Establecer formato de razonamiento seg√∫n el tipo de modelo actual
                current_razonamiento_formato = None
                if current_tipo_modelo == "Razonamiento":
                    current_razonamiento_formato = "raw"
                
                # Obtener respuesta del modelo con streaming (sin memoria)
                response = get_response_streaming(messages_for_api, response_placeholder, current_razonamiento_formato, current_tipo_modelo)
                
                # Limpiar cualquier etiqueta HTML antes de guardar en el historial
                html_patterns = [
                    "</div>", 
                    '<div class="chat-text">', 
                    '<div class=', 
                    '</div>', 
                    '<span', 
                    '</span>'
                ]
                
                # Verificar si el contenido tiene etiquetas HTML incorrectas
                has_html_tags = any(pattern in response for pattern in html_patterns)
                
                # Limpiar la respuesta si es necesario
                clean_response = response
                if has_html_tags:
                    for pattern in html_patterns:
                        clean_response = clean_response.replace(pattern, '')
                
                # Eliminar espacios extras y asegurarse de que la respuesta est√© limpia
                clean_response = clean_response.strip()
                
                # Procesar el mensaje para asegurar un formato correcto de etiquetas <think>
                processed_response = procesar_mensaje_razonamiento(clean_response)
                
                # Guardar la respuesta procesada en el historial
                st.session_state.messages.append({"role": "assistant", "content": processed_response})

# Mostrar los √∫ltimos par√°metros utilizados si existen
if 'last_request_params' in st.session_state:
    st.sidebar.markdown("---")
    st.sidebar.subheader("√öltima solicitud")
    st.sidebar.text(f"Modelo: {st.session_state['last_request_params']['modelo']}")
    st.sidebar.text(f"Temperatura: {st.session_state['last_request_params']['temperatura']}")
    st.sidebar.text(f"Tokens m√°ximos: {st.session_state['last_request_params']['max_tokens']}")

# Informaci√≥n sobre c√≥mo obtener API key de Groq
if st.session_state.pagina_actual == 'configuracion':
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    ### Configuraci√≥n de la API key
    Para mayor seguridad, guarda tu API key en un archivo .env:
    1. Crea un archivo llamado `.env` en la ra√≠z del proyecto
    2. A√±ade la l√≠nea `GROQ_API_KEY=tu_api_key_aqu√≠`
    3. Este archivo est√° incluido en .gitignore y no se subir√° a Github
    """) 
    
    # A√±adir informaci√≥n sobre Tavily Search API
    if tavily_available:
        st.sidebar.markdown("---")
        st.sidebar.markdown("""
        ### Habilitar b√∫squeda web (opcional)
        Para permitir que el asistente busque informaci√≥n en internet:
        1. Obt√©n tu API key gratuita en [Tavily Search API](https://tavily.com)
        2. A√±ade la l√≠nea `TAVILY_API_KEY=tu_api_key_aqu√≠` en tu archivo `.env`
        3. La herramienta de b√∫squeda se activar√° autom√°ticamente
        
        Con esta funcionalidad, el asistente podr√° buscar informaci√≥n actualizada en tiempo real cuando sea necesario.
        """, unsafe_allow_html=True) 